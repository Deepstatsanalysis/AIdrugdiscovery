{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e91a7dbdfc32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcsv_f\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, DataStructs\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from sklearn.externals import joblib\n",
    "# dense to sparse\n",
    "from numpy import array\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from keras import regularizers\n",
    "abscaler= preprocessing.MaxAbsScaler()\n",
    "from os import environ\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "f = open('csvfile.csv')\n",
    "csv_f = csv.reader(f)\n",
    "i=0\n",
    "args=[]\n",
    "for row in csv_f:\n",
    "    args+=row\n",
    "    \n",
    "nodes=np.int64(args[0])  \n",
    "dropout=float(args[1])\n",
    "regularizers=float(args[2])\n",
    "lr=float(args[3])\n",
    "momentum=float(args[4])\n",
    "decay=float(args[5])\n",
    "mynesterov=args[6]\n",
    "theloss=args[7]\n",
    "epochs=np.int64(args[8])\n",
    "batchsize=np.int64(args[9])\n",
    "filename=args[10]\n",
    "modelname=args[11]\n",
    "\n",
    "def process(filename,modelname, dropout, nodes, regularizers, epochs, batchsize, lr, momentum, decay, mynesterov):\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    dataset = open(filename, \"r\" )\n",
    "    dataset = [ line.rstrip().split(\",\") for line in dataset ][0:]\n",
    "    mols = [ Chem.MolFromSmiles( line[0] ) for line in dataset ]\n",
    "\n",
    "\n",
    "    u=0\n",
    "    indexy=[]\n",
    "    for y in mols:\n",
    "        if y is not None:\n",
    "            indexy.append(u)\n",
    "            u+=1\n",
    "        \n",
    "    goodmols=[mols[k] for k in indexy]\n",
    "    \n",
    "   \n",
    "\n",
    "    Y=[line[1] for line in dataset]\n",
    "    goody=[Y[k] for k in indexy]\n",
    "    trainmols, testmols, trainy, testy = train_test_split(goodmols, goody, test_size = 0.1, random_state=90  )\n",
    "    #trainmols, testmols, trainy, testy = train_test_split(goodmols, Y, test_size = 0.1, random_state=90  )\n",
    "    trainfps = [AllChem.GetMorganFingerprintAsBitVect(m, 2) for m in goodmols if m is not None ]\n",
    "    testfps = [AllChem.GetMorganFingerprintAsBitVect(m, 2) for m in testmols if m is not None]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    u=0\n",
    "    indexy=[]\n",
    "    for y in trainfps:\n",
    "        if y is not None:\n",
    "            indexy.append(u)\n",
    "            u+=1\n",
    "    \n",
    "    newy=array([int(goody[k]) for k in indexy])\n",
    "    print(len(newy))\n",
    "    \n",
    "    np_fptrain=[]\n",
    "    for fp in trainfps:\n",
    "        arr=np.zeros((1,))\n",
    "        DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "        np_fptrain.append(arr)\n",
    "\n",
    "    np_fptest=[]\n",
    "    for fp in testfps:\n",
    "        arr=np.zeros((1,))\n",
    "        DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "        np_fptest.append(arr)  \n",
    "    \n",
    "    \n",
    "    a=csr_matrix(np_fptrain, dtype=np.int8).toarray()\n",
    "    b=csr_matrix(np_fptest, dtype=np.int8).toarray()\n",
    "    a=abscaler.fit_transform(a)\n",
    "    b=abscaler.fit_transform(b)\n",
    "    \n",
    "    print(len(a))\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "    from sklearn import datasets\n",
    "\n",
    "   \n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.model_selection import cross_validate\n",
    "    from keras.models import Sequential\n",
    "    from keras import layers\n",
    "    from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    from keras.models import Sequential\n",
    " \n",
    "    from keras.callbacks import ReduceLROnPlateau\n",
    "    from keras import regularizers as WeightRegularizer\n",
    "    from keras.optimizers import SGD\n",
    "    #SKlearn for metrics and datasplits\n",
    "    from sklearn.model_selection import cross_validate\n",
    "    from sklearn.metrics import roc_auc_score, roc_curve\n",
    "    #Matplotlib for plotting\n",
    "    from matplotlib import pyplot as plt\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(nodes, init='uniform', activation='relu', kernel_regularizer=keras.regularizers.l2(l=regularizers)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.Dense(nodes, activation='relu', kernel_regularizer=keras.regularizers.l2(l=regularizers)))\n",
    "    \n",
    "    model.add(layers.Dense(1,  activation='sigmoid'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    sgd=SGD(lr, momentum, decay, nesterov=mynesterov)\n",
    "    model.compile(loss=theloss, optimizer=sgd, metrics=['accuracy'])\n",
    "    model.fit(a, newy, nb_epoch=epochs, batch_size=batchsize)\n",
    "    \n",
    "    themodel=modelname + '.h5'\n",
    "    model.save(themodel)\n",
    "   \n",
    "    from keras.models import load_model\n",
    "    model = load_model('classifier.h5')\n",
    "    model.compile(optimizer = 'sgd', loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "    from sklearn.metrics import (matthews_corrcoef, \n",
    "                             confusion_matrix, \n",
    "                             f1_score, \n",
    "                             roc_auc_score,\n",
    "                             accuracy_score,\n",
    "                             roc_auc_score)\n",
    "    predy=model.predict(b)\n",
    "    predy[predy > 0.5] = 1\n",
    "    predy[predy <= 0.5] = 0\n",
    "    testy=np.delete(testy, -1)\n",
    "    testb=[int(h) for h in testy]\n",
    "                \n",
    "    tn, fp, fn, tp = confusion_matrix(testb, predy).ravel()\n",
    "    sensitivity=tp/(tp + fn)\n",
    "    specificity= tn/(tn + fp)\n",
    "    accuracy=(tp+tn)/(tn+fp + fn +tp)\n",
    "                \n",
    "    sample = open('samplefile.txt', 'w') \n",
    "                \n",
    "                \n",
    "    print(\"accuracy\", accuracy, file=sample)\n",
    "    print(\"sensitivity\", sensitivity, file=sample)\n",
    "    print(\"specificity\", specificity, file=sample)\n",
    "                \n",
    "    sample.close() \n",
    "##########################################\n",
    "    \n",
    "\n",
    "process(filename,modelname, dropout, nodes, regularizers, epochs, batchsize, lr, momentum, decay, mynesterov)   \n",
    "    \n",
    "    \n",
    "    \n",
    " \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.models import load_model\n",
    "#model = load_model('classifier.h5')\n",
    "#model.compile(optimizer = 'sgd', loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9936490655053529\n",
      "sensitivity 0.7652173913043478\n",
      "specificity 0.9985174203113417\n"
     ]
    }
   ],
   "source": [
    "#tn=5388\n",
    "#fp=8\n",
    "#fn=27\n",
    "#tp=88\n",
    "#sensitivity=tp/(tp + fn)\n",
    "#specificity= tn/(tn + fp)\n",
    "#accuracy=(tp+tn)/(tn+fp + fn +tp)\n",
    "#print(\"accuracy\", accuracy)\n",
    "#print(\"sensitivity\", sensitivity)\n",
    "#print(\"specificity\", specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#import numpy \n",
    "#newdf=pd.read_csv('gsk3new.txt', sep='\\t', names=['smiles', 'value'])\n",
    "#def predictor(myinput):\n",
    "#    smile=myinput\n",
    "        \n",
    "#    themol=Chem.MolFromSmiles(smile)\n",
    "#    estimator = load_model('classifier.h5')\n",
    "    \n",
    "#    fp = AllChem.GetMorganFingerprintAsBitVect(themol,2,nBits=2048)\n",
    "#    res = numpy.zeros(len(fp),numpy.int32)\n",
    "#    DataStructs.ConvertToNumpyArray(fp,res)\n",
    "#    probas = list(estimator.predict(res.reshape(1,-1))[0])\n",
    "#    print(probas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from rdkit import Chem\n",
    "#import numpy as np\n",
    "#from rdkit.Chem import AllChem, DataStructs\n",
    "#validate=newdf['smiles']\n",
    "#testvalidate=newdf['value']\n",
    "#mypreds=[]\n",
    "#for y in validate:\n",
    "#    a=predictor(y)\n",
    "#    mypreds.append(a)\n",
    "#print(mypreds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00056889275]\n",
      "[0.011415197]\n",
      "[0.00022030345]\n",
      "[0.31587937]\n",
      "[0.00012703771]\n",
      "[0.16461712]\n",
      "[0.038860872]\n",
      "[0.00059711566]\n",
      "[0.43126932]\n",
      "[0.012150672]\n"
     ]
    }
   ],
   "source": [
    "#from rdkit import Chem\n",
    "#import numpy as np\n",
    "#from rdkit.Chem import AllChem, DataStructs\n",
    "#newdf=pd.read_csv('testset.txt', names=['smiles'])\n",
    "#validate=newdf['smiles']\n",
    "#for h in validate:\n",
    "#    predictor(h)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
